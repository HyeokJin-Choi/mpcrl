// https://github.com/SamuelMallick/mpcrl-greenhouse 를 기반으로 연구를 진행.
상추(Lettuce) 재배 온실 환경을 시뮬레이션하면서, 그 안에 **MPC(Model Predictive Control)**와 **강화학습(RL)**을 결합해 제어 성능을 개선하는 연구 코드

*** 환경 모델 ***
LettuceGreenHouse라는 환경 클래스를 정의해서, 온실 내 상태(온도, 습도, CO₂, 빛 등)와 외부 교란(날씨, 일사량 등)을 모사.
*** 재배 작물 *** 
상추 (lettuce).
온실에서 상추가 자라는 과정을 수학적/물리적 모델로 시뮬레이션.
하루 단위(num_days)로 성장 과정을 반영.
*** 제어 목표 ***
상추가 잘 자랄 수 있는 최적 환경(예: 온도·습도·CO₂ 농도 유지) 유지.
동시에 에너지 소비(히터, 팬, CO₂ 주입 등) 최소화.
*** 도입된 기법 ***
MPC: 물리적 모델 기반 예측 제어 → 안정성 보장.
강화학습: MPC의 모델 파라미터를 학습·보정하여 더 나은 제어 성능 달성.

*** MPC란? ***
MPC (Model Predictive Control)
<개념>
MPC는 “예측 제어” 기법으로, 시스템의 수학적 모델을 기반으로 일정 시간 구간(horizon) 동안 미래 상태를 예측하고, **비용 함수(cost function)**를 최소화하는 최적의 제어 입력 시퀀스를 계산.
<특징>
제약 고려 가능: 제어 입력 크기, 변화율, 상태 제약 등을 수학적으로 넣을 수 있음.
짧은 horizon 반복: 매 스텝마다 새로운 최적화를 풀고 첫 번째 입력만 적용 → 다음 스텝에서 다시 반복.
안정성·안전성: 물리 모델을 기반으로 하므로 제어 신호가 안정적이고 제약 조건을 만족시킬 수 있음.

*** MPC + RL? ***
RL은 MPC 단독 제어의 한계를 보완하기 위해 도입.
RL이 학습하는 건 MPC가 쓰는 모델/비용 함수 파라미터.
즉, RL은 환경과 상호작용하며 MPC의 내부 파라미터(예: 모델 불확실성 보정, 비용 가중치, 교란 적응 파라미터 등)를 업데이트.
실제 온실에서는 정확한 모델링이 어렵고, 외부 환경(날씨, 교란)이 불확실함.
순수 MPC는 잘못된 모델/파라미터를 쓰면 성능이 급격히 떨어짐.
RL은 trial-and-error를 통해 이 파라미터들을 적응적으로 학습 → MPC가 더 현실에 맞게 작동.
<역할 분담>
MPC: 안정적이고 제약을 고려한 최적 제어(단기 horizon 예측).
RL: 장기적인 누적 보상 극대화를 통해 MPC의 파라미터를 조정, 모델 불확실성과 외부 교란에 대한 적응력 확보.


-----<Code Analysis>-----
*** q_learning_greenhouse.py ***
# 시뮬레이션 환경(LettuceGreenHouse)을 초기화할 때 주어지는 파라미터들. 즉, 실제 상추 재배 과정을 모사하는 “가상 온실 환경”의 조건을 어떻게 설정할지를 지정하는 부분.
LettuceGreenHouse(
    growing_days=test.num_days,              # 상추를 키우는 총 기간 (재배일수, 시뮬레이션 horizon)
    model_type=test.base_model,              # 어떤 물리/수학 모델을 쓸지 (예: euler vs rk4, 단순 모델 vs 정밀 모델)
    cost_parameters_dict=test.rl_cost,       # 제어 비용/보상 함수 파라미터 (에너지 소비, 작물 성장 효율 등 가중치)
    disturbance_profiles_type=test.disturbance_type,  # 외부 교란(날씨 패턴) 시나리오 선택
    noisy_disturbance=test.noisy_disturbance,# 교란에 노이즈를 추가할지 여부 (현실성 ↑)
    clip_action_variation=test.clip_action_variation  # 제어 입력 변화량을 제한할지 여부 (안정성 ↑)
)

ex) 
- growing_days=90이면 90일간의 온실 운영을 시뮬레이션.
- disturbance_profiles_type="weather_based"라면 실제 기상 데이터를 반영.
- rl_cost에 따라 보상 함수가 달라져서 RL이 학습하는 목표(작물 품질 vs 에너지 절감)가 바뀜.
- clip_action_variation=True이면 제어 신호가 갑자기 튀지 않게 제한.


*** visualization.py ***
# 아래의 코드의 결과로 얻는 파일을 순서로 설명하자면.
for i, fig in enumerate(plt.get_fignums()):
    plt.figure(fig)
    plt.savefig(f"test/figure_{i}.png", dpi=200)

<figure_0.png>
"Timestep-wise Learning (figure_0.png)"
X축: 시뮬레이션 타임스텝
Y축: 학습 관련 항목들
Lₜ : 스텝별 비용/손실 함수 값
δₜ : 스텝별 파라미터 업데이트 크기 (gradient step 크기와 유사)
violₜ : 제약 위반 정도 (온도/습도 범위 밖으로 나간 정도)
👉 의미: 매 타임스텝마다 RL+MPC가 얼마나 손실을 줄이고 있는지, 제약 위반이 언제 발생했는지 보여줌.

<figure_1.png>
"Episode-wise Learning (figure_1.png)"
X축: Episode (학습 반복)
여러 지표들이 에피소드 단위로 집계됨:
Lₑₚ : 에피소드 전체 비용
δₑₚ : 에피소드 전체 업데이트 크기
violₛₑₚ, viols^{du}_{ep} : 제약 위반 횟수/크기
EPI : Energy Performance Index (에너지 효율 지표)
yield : 작물 수확량 지표
👉 의미: 에피소드가 진행될수록 RL이 비용을 줄이고, 제약 위반을 줄이고, yield와 에너지 효율이 어떻게 변하는지 보여줌.

<figure_2.png>
"Cost Parameters (figure_2.png)"
Y축: 비용 함수(cost function)에 쓰이는 파라미터 값
예: c_dy, c_y, c_u 등은 각각 상태 오차, 출력 추종, 제어 입력 변화에 대한 가중치
👉 의미: RL이 학습하면서 MPC의 비용 함수 가중치를 어떻게 업데이트했는지 확인 가능.
→ 즉, RL이 “온도 추종을 더 중시할까, 에너지 절약을 더 중시할까” 같은 의사결정을 조정한 흔적.

<figure_3.png>
"Dynamics Parameters (figure_3.png)"
모델 파라미터(p_0, p_1, ..., p_27)들이 학습 과정에서 어떻게 업데이트되는지 보여줌.
이는 온실 환경의 동역학(열/습도/CO₂ 이동 방정식) 계수들을 의미.
👉 의미: RL이 환경 모델 파라미터를 보정하면서, MPC가 현실적/적응적인 모델을 쓰도록 도와줌.

<figure_4.png>
"Outputs (figure_4.png)"
좌측: 첫 번째 에피소드, 우측: 마지막 에피소드
파란선: 내부 상태 변수(온실 출력, 예: 온도/습도/CO₂ 농도)
빨간선: 상한 제약, 검은선: 하한 제약
👉 의미: RL+MPC 학습이 진행됨에 따라, 내부 상태가 점점 제약 범위 안에서 안정적으로 유지되는지 비교 가능.

<figure_5.png>
"Control Actions (figure_5.png)"
좌측: 첫 번째 에피소드, 우측: 마지막 에피소드
파란선: 제어 입력(히터, 팬, CO₂ 주입기 등)
빨간/검은선: 제어 입력 제약(상/하한)
👉 의미: RL+MPC 학습이 진행되면서 제어 입력이 제약을 덜 위반하고, 더 효율적으로 사용되는지 비교 가능.

<figure_6.png>
"Disturbances (figure_6.png)"
외부 환경(태양 복사, 외기 온도, 외기 습도, 외기 CO₂ 등) 시계열
👉 의미: RL+MPC가 대응해야 했던 외부 조건을 보여줌. 이 조건 위에서 위의 Outputs/Actions이 나온 것.

<요약>
figure_0~1 : RL 학습 곡선 (스텝/에피소드 단위)
figure_2~3 : RL이 튜닝한 MPC 비용 함수/동역학 파라미터
figure_4~5 : RL+MPC 제어 결과 (내부 상태 / 제어 입력)
figure_6 : 외부 교란 (날씨 조건)
즉, 한 세트로 보면:
Disturbances(조건) → Actions(제어) → Outputs(내부 상태) → Learning curves(학습 성능) → Parameter adaptation(RL이 MPC 보조)

