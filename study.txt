// https://github.com/SamuelMallick/mpcrl-greenhouse 를 기반으로 연구를 진행.
상추(Lettuce) 재배 온실 환경을 시뮬레이션하면서, 그 안에 **MPC(Model Predictive Control)**와 **강화학습(RL)**을 결합해 제어 성능을 개선하는 연구 코드

*** 환경 모델 ***
LettuceGreenHouse라는 환경 클래스를 정의해서, 온실 내 상태(온도, 습도, CO₂, 빛 등)와 외부 교란(날씨, 일사량 등)을 모사.
*** 재배 작물 *** 
상추 (lettuce).
온실에서 상추가 자라는 과정을 수학적/물리적 모델로 시뮬레이션.
하루 단위(num_days)로 성장 과정을 반영.
*** 제어 목표 ***
상추가 잘 자랄 수 있는 최적 환경(예: 온도·습도·CO₂ 농도 유지) 유지.
동시에 에너지 소비(히터, 팬, CO₂ 주입 등) 최소화.
*** 도입된 기법 ***
MPC: 물리적 모델 기반 예측 제어 → 안정성 보장.
강화학습: MPC의 모델 파라미터를 학습·보정하여 더 나은 제어 성능 달성.

*** MPC란? ***
MPC (Model Predictive Control)
<개념>
MPC는 “예측 제어” 기법으로, 시스템의 수학적 모델을 기반으로 일정 시간 구간(horizon) 동안 미래 상태를 예측하고, **비용 함수(cost function)**를 최소화하는 최적의 제어 입력 시퀀스를 계산.
<특징>
제약 고려 가능: 제어 입력 크기, 변화율, 상태 제약 등을 수학적으로 넣을 수 있음.
짧은 horizon 반복: 매 스텝마다 새로운 최적화를 풀고 첫 번째 입력만 적용 → 다음 스텝에서 다시 반복.
안정성·안전성: 물리 모델을 기반으로 하므로 제어 신호가 안정적이고 제약 조건을 만족시킬 수 있음.

*** MPC + RL? ***
RL은 MPC 단독 제어의 한계를 보완하기 위해 도입.
RL이 학습하는 건 MPC가 쓰는 모델/비용 함수 파라미터.
즉, RL은 환경과 상호작용하며 MPC의 내부 파라미터(예: 모델 불확실성 보정, 비용 가중치, 교란 적응 파라미터 등)를 업데이트.
실제 온실에서는 정확한 모델링이 어렵고, 외부 환경(날씨, 교란)이 불확실함.
순수 MPC는 잘못된 모델/파라미터를 쓰면 성능이 급격히 떨어짐.
RL은 trial-and-error를 통해 이 파라미터들을 적응적으로 학습 → MPC가 더 현실에 맞게 작동.
<역할 분담>
MPC: 안정적이고 제약을 고려한 최적 제어(단기 horizon 예측).
RL: 장기적인 누적 보상 극대화를 통해 MPC의 파라미터를 조정, 모델 불확실성과 외부 교란에 대한 적응력 확보.


-----<Code Analysis>-----
*** q_learning_greenhouse.py ***
# 시뮬레이션 환경(LettuceGreenHouse)을 초기화할 때 주어지는 파라미터들. 즉, 실제 상추 재배 과정을 모사하는 “가상 온실 환경”의 조건을 어떻게 설정할지를 지정하는 부분.
LettuceGreenHouse(
    growing_days=test.num_days,              # 상추를 키우는 총 기간 (재배일수, 시뮬레이션 horizon)
    model_type=test.base_model,              # 어떤 물리/수학 모델을 쓸지 (예: euler vs rk4, 단순 모델 vs 정밀 모델)
    cost_parameters_dict=test.rl_cost,       # 제어 비용/보상 함수 파라미터 (에너지 소비, 작물 성장 효율 등 가중치)
    disturbance_profiles_type=test.disturbance_type,  # 외부 교란(날씨 패턴) 시나리오 선택
    noisy_disturbance=test.noisy_disturbance,# 교란에 노이즈를 추가할지 여부 (현실성 ↑)
    clip_action_variation=test.clip_action_variation  # 제어 입력 변화량을 제한할지 여부 (안정성 ↑)
)

ex) 
- growing_days=90이면 90일간의 온실 운영을 시뮬레이션.
- disturbance_profiles_type="weather_based"라면 실제 기상 데이터를 반영.
- rl_cost에 따라 보상 함수가 달라져서 RL이 학습하는 목표(작물 품질 vs 에너지 절감)가 바뀜.
- clip_action_variation=True이면 제어 신호가 갑자기 튀지 않게 제한.

